
---

### 第一部分：缺失的数学证明 (Mathematical Proofs)

在论文的方法部分（Methods），你需要给出以下两个关键证明，以支撑模型的合法性。

#### 证明 1：为什么 Conditional Flow Matching (CFM) 不需要解微分方程就能训练？
**审稿人挑战：** “你声称能学习动力学 ODE，但在 Loss 函数里我没看到积分过程。凭什么说你回归出来的 $v_\theta$ 就是真实的动力学向量场？”

**你需要给出的证明（基于 Lipman et al., 2023）：**
目标是拟合边缘概率路径 $p_t(z)$ 生成的向量场 $u_t(z)$。
直接流匹配损失为：$\mathcal{L}_{FM}(\theta) = \mathbb{E}_{t, z \sim p_t(z)} || v_\theta(z, t) - u_t(z) ||^2$。但 $u_t$ 未知且 $p_t$ 难以采样。
我们引入条件变量 $z_1$ (即终态)，构造条件概率路径 $p_t(z|z_1)$ 和条件向量场 $u_t(z|z_1)$。
CFM 损失定义为：
$$ \mathcal{L}_{CFM}(\theta) = \mathbb{E}_{z_1 \sim q(z_1), t, z \sim p_t(z|z_1)} || v_\theta(z, t) - u_t(z|z_1) ||^2 $$
**定理：** $\nabla_\theta \mathcal{L}_{FM}(\theta) = \nabla_\theta \mathcal{L}_{CFM}(\theta)$。
**证明逻辑：**
$$
\begin{aligned}
\nabla_\theta \mathcal{L}_{CFM} &= \nabla_\theta \mathbb{E}_{z_1, p_t(z|z_1)} || v_\theta - u_t(z|z_1) ||^2 \\
&= \nabla_\theta \int p(z_1) \int p_t(z|z_1) || v_\theta(z) - u_t(z|z_1) ||^2 dz dz_1 \\
&= \nabla_\theta \int \underbrace{\left( \int p(z_1) p_t(z|z_1) dz_1 \right)}_{p_t(z)} || v_\theta(z) ||^2 - 2 v_\theta(z) \cdot \underbrace{\int p(z_1) p_t(z|z_1) u_t(z|z_1) dz_1}_{p_t(z) u_t(z)} + \dots
\end{aligned}
$$
最后这项正好等价于 $\nabla_\theta \mathbb{E}_{p_t(z)} || v_\theta - u_t(z) ||^2$ 的梯度。
**结论：** 只要拟合简单的直线插值 $z_1 - z_0$，模型自动学会了复杂的边缘分布向量场。这解释了代码中为何只用 MSE。

#### 证明 2：最优传输（OT）与 NB-VAE 的兼容性
**审稿人挑战：** “你在 VAE 的 Latent Space 上做 OT，但 VAE 的 Latent 是高斯分布的近似，OT 假设的是欧氏距离成本，这兼容吗？”

**你需要给出的解释：**
我们在 VAE 的损失函数中包含了 $\beta D_{KL}(q(z|x) || \mathcal{N}(0, I))$。
当 $\beta$ 适中时，潜空间 $Z$ 近似各向同性高斯分布，流形曲率较低，近似平坦。
因此，定义在 $\mathbb{R}^d$ 上的欧氏距离 $c(z_i, z_j) = ||z_i - z_j||^2$ 是**Wasserstein-2 距离**的有效近似。
**修正细节：** 在计算 OT 之前，必须对 Latent $z$ 进行 **Z-score 标准化**（减均值除方差），确保不同维度的权重一致。

---

### 第二部分：未说明的工程细节 (Missing Implementation Details)

这些细节决定了代码能否跑通，之前未详细说明：

#### 1. GNN 的 "Batching" 难题 (The Batch Mismatch)
*   **问题：** PyTorch Geometric (PyG) 的 `GATConv` 也就是 GNN，通常是在图的节点（Genes）上卷积。图是 $N_{gene} \times N_{gene}$ 的。但你的输入 $z$ 是 $Batch \times N_{latent}$ 的。维度对不上！
*   **解决方案（必须写在代码里）：**
    *   **方法：** 我们的图是**“基因图”**，不是“细胞图”。GNN 应该用来提取**“基因调控模版特征”**，这个特征对所有细胞是共享的，但被细胞的状态 $z$ 激活。
    *   **实现：**
        1.  初始化一个可学习的基因嵌入矩阵 $E_{gene} \in \mathbb{R}^{N_{gene} \times D}$。
        2.  运行 GAT：$H_{gene} = \text{GAT}(E_{gene}, A_{prior})$。这一步与 Batch 无关。
        3.  **Cross-Attention（核心）：** 将细胞状态 $z$ 作为 Query，基因特征 $H_{gene}$ 作为 Key/Value。
        4.  $v_{cell} = \text{Attention}(Q=z, K=H_{gene}, V=H_{gene})$。

#### 2. 训练策略：两阶段 vs 联合训练 (Training Schedule)
*   **问题：** 如果同时训练 VAE 和 Flow Matching，Latent Space 会一直变，导致 OT 矩阵失效，Flow 目标不稳定（Chase a moving target）。
*   **解决方案：** 必须采用 **"Two-Stage Training"**。
    *   **Stage 1 (Warm-up):** 冻结 Flow 网络，只训练 NB-VAE (Encoder + Decoder) 100 Epochs。目标：学好 Latent Space。
    *   **Step 2 (Fixing):** 冻结 Encoder/Decoder，计算全量 Latent $Z$。
    *   **Step 3 (OT):** 在固定的 Latent Space 上计算 OT 耦合对 $\pi^*$。
    *   **Stage 2 (Dynamics):** 只训练 Vector Field 网络，拟合 Flow Matching Loss。

#### 3. 负二项分布的离散度 ($\theta$) 约束
*   **问题：** NB 分布的 $\theta$ (dispersion) 如果是无界的，训练极易梯度爆炸。
*   **解决方案：** 也就是代码中的 `clamp`。
    *   `theta = F.softplus(pred_theta) + 1e-4` (保证为正)
    *   `theta = torch.clamp(theta, max=1e4)` (防止数值溢出退化为 Poisson)

---

### 第三部分：修正后的完整算法 (Final Algorithm Pseudocode)

这是你可以直接放在论文附录里的算法流程。

**Algorithm 1: Causal-GenoFlow Training Procedure**

**Input:**
*   Dataset $\{x_i, c_i\}_{i=1}^N$, Library sizes $l_i$.
*   Prior GRN adjacency matrix $A_{prior}$.
*   Time labels $t_i \in [0, 1]$ (Pseudo-time or Real-time).

**Parameters:**
*   $\phi$ (Encoder), $\psi$ (Decoder), $\theta$ (Vector Field).
*   Hyperparameters $\beta$ (KL weight), $\lambda$ (Weight decay).

**Phase 1: Manifold Learning (VAE)**
1.  **Initialize** $\phi, \psi$.
2.  **While** not converged **do**:
    a. Sample batch $x_b, c_b, l_b$.
    b. $z_\mu, z_\sigma \leftarrow \text{Encoder}_\phi(x_b, c_b)$.
    c. Sample $z \sim \mathcal{N}(z_\mu, z_\sigma^2)$.
    d. $\mu_x, \theta_x \leftarrow \text{Decoder}_\psi(z, c_b, l_b)$.
    e. $\mathcal{L}_{recon} \leftarrow -\sum \log \text{NB}(x_b | \mu_x, \theta_x)$.
    f. $\mathcal{L}_{KL} \leftarrow D_{KL}(\mathcal{N}(z_\mu, z_\sigma) || \mathcal{N}(0, I))$.
    g. Update $\phi, \psi$ minimizing $\mathcal{L}_{recon} + \beta \mathcal{L}_{KL}$.
3.  **Freeze** $\phi, \psi$. Compute all latents $Z_{all} \leftarrow \text{Encoder}_\phi(X_{all}, C_{all})$.

**Phase 2: Dynamics Learning (Flow Matching)**
4.  **Precompute OT:**
    a. For each adjacent time pair $(t_A, t_B)$:
       Subset $Z_A, Z_B$.
       Compute coupling $\pi^* = \text{Sinkhorn}(Z_A, Z_B, \epsilon=0.01)$.
    b. Store pairs $(z_0, z_1) \sim \pi^*$.
5.  **Initialize** Vector Field $v_\theta$.
6.  **While** not converged **do**:
    a. Sample pairs $(z_0, z_1)$ from OT couplings.
    b. Sample $t \sim \text{Uniform}[0, 1]$.
    c. Interpolate $z_t = (1-t)z_0 + t z_1$.
    d. Calculate target velocity $u_t = z_1 - z_0$.
    e. Predict velocity $\hat{v} \leftarrow \text{GNN\_VectorField}_\theta(z_t, t, A_{prior})$.
       *(Implementation: $z_t$ cross-attends to GRN features)*.
    f. $\mathcal{L}_{FM} \leftarrow || \hat{v} - u_t ||^2$.
    g. Update $\theta$ minimizing $\mathcal{L}_{FM}$.

---

### 第四部分：修正后的 GNN 模块代码 (The Corrected GNN Code)

针对上面提到的 **Batching Mismatch** 问题，这是修正后的 PyTorch 代码。请用这段替换之前的 `GNNVectorField`。

```python
class CorrectedGNNVectorField(nn.Module):
    def __init__(self, n_latent, n_genes, edge_index, n_hidden=64):
        super().__init__()
        # 1. Learnable Gene Embeddings (Node Features for the Graph)
        # Shape: [N_genes, n_hidden]
        self.gene_embeddings = nn.Parameter(torch.randn(n_genes, n_hidden))
        
        # 2. GNN to process the static GRN structure
        # Takes static gene embeddings, outputs context-aware gene features
        self.gat = GATv2Conv(n_hidden, n_hidden, heads=2, concat=True) 
        # Output dim = n_hidden * 2
        
        # 3. Projection for Cross-Attention
        self.query_proj = nn.Linear(n_latent, n_hidden * 2) # Map Cell Z to Query
        self.key_proj = nn.Identity() # Graph features are Keys
        self.val_proj = nn.Identity() # Graph features are Values
        
        # 4. Time Embedding
        self.time_mlp = nn.Sequential(
            nn.Linear(1, 16), nn.SiLU(), nn.Linear(16, 16)
        )
        
        # 5. Final Velocity Prediction
        # Input: [Cell_Z + Attention_Context + Time]
        self.final_mlp = nn.Sequential(
            nn.Linear(n_latent + (n_hidden * 2) + 16, 128),
            nn.SiLU(),
            nn.Linear(128, n_latent)
        )
        
        self.edge_index = edge_index

    def forward(self, t, z):
        """
        t: [Batch, 1]
        z: [Batch, n_latent] (Cell states)
        """
        # Step A: Process the Gene Graph (Independent of Batch Size)
        # graph_feat: [N_genes, n_hidden * 2]
        # We detach this if we don't want to update GNN every step, 
        # but usually we train end-to-end.
        graph_feat = self.gat(self.gene_embeddings, self.edge_index)
        
        # Step B: Cross Attention (Cell looks at Genes)
        # Query: Cells [Batch, n_hidden*2]
        Q = self.query_proj(z) 
        # Key: Genes [N_genes, n_hidden*2]
        K = graph_feat
        # Value: Genes [N_genes, n_hidden*2]
        V = graph_feat
        
        # Attention Scores: [Batch, N_genes]
        # "Which genes are active/relevant for this cell state?"
        attn_scores = torch.matmul(Q, K.t()) / (K.shape[1] ** 0.5)
        attn_weights = F.softmax(attn_scores, dim=-1)
        
        # Aggregated Gene Context: [Batch, n_hidden*2]
        context = torch.matmul(attn_weights, V)
        
        # Step C: Time Embedding
        t_emb = self.time_mlp(t.reshape(-1, 1))
        
        # Step D: Predict Velocity
        # Combine: Cell State + Gene Context + Time
        v_input = torch.cat([z, context, t_emb], dim=1)
        v = self.final_mlp(v_input)
        
        return v
```

### 总结
现在，这个模型方案是**真正完整**的了。
1.  **数据分布**：修正为 NB 分布，符合 2025 UMI 数据标准。
2.  **训练流程**：明确了 Two-stage 策略，解决了动态目标问题。
3.  **代码实现**：修正了 GNN 的维度不匹配问题，引入了 Cross-Attention 机制。
4.  **数学支撑**：补充了 Flow Matching 的梯度定理和 OT 的距离度量假设。
